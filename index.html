<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Raghav Singhal</title>

    <meta name="author" content="Raghav Singhal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Raghav Singhal
                </p>
                I'm currently an AI PhD student in the Computer Science department at EPFL. 
                I am fortunate to be advised by the amazing <a href=https://dlab.epfl.ch/people/west/">Prof. Robert West</a> and am a contributor to the <a href="https://huggingface.co/collections/swiss-ai/apertus-llm">Apertus project</a> (the biggest fully open and compliant training run & LLM to date).<br>
                <br>
                Previously, I was a researcher at Massachusetts Institute of Technology and Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), where I worked with <a href="https://praneeth.mit.edu/">Prof. Praneeth Vepakomma</a>. 
                Prior to this, I graduated from IIT Bombay with a Bachelor's in EE and a Master's in AI/ML.
                <!-- <p>I'm a research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>. -->
                </p>
                <hr style="border: 0; border-top: 1px solid #ccc; margin: 20px 0;">
                <p>
                  For EPFL students: If you are interested in a project, please feel free to reach out via mail. I'm very happy to supervise motivated students!
                </p>
                <!-- <p> -->
                  <!-- At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
                <!-- </p> -->
                <p style="text-align:center">
                  <a href="mailto:10raghavsinghal@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Nc4_zNIAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/ragghhavvv">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/raghav-singhal-a6666b148/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/RaghavSinghal10">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images_raghav/RaghavSinghal.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images_raghav/RaghavSinghal.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I spend my time these days working toward improving the foundations of foundation models, and studying fundamental flaws in current approaches to safety & alignment in LLMs.
                </p>
                <p>
                  Some of the projects I am currently working on include:
                </p>
                <ul style="padding-left: 20px; margin-top: 0; margin-bottom: 0;">
                  <li style="margin-bottom: 8px;">Instilling safety in LLMs during pretraining to create a strong prior for post-training, rather than relying on current post-training only approaches that are hacky and easy to break.</li>
                  <li>Improving the faithfulness of chain-of-thought reasoning in LLMs.</li>
                </ul>
                <p>
                  Check out my <a href="https://scholar.google.com/citations?user=Nc4_zNIAAAAJ&hl=en">Google Scholar</a> for a complete list of publications.
                  * denotes equal contribution.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/apertus.png" alt="Apertus" width="100%" height="100%" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2509.14233" id="MCG_journal">
            <span class="papertitle">Apertus: Democratizing Open and Compliant LLMs for Global Language Environments
            </span>
            </a>
            <br>
            Project Apertus
            <br>
            <a href="https://arxiv.org/abs/2509.14233">arXiv</a> /
            <a href="https://huggingface.co/collections/swiss-ai/apertus-llm-68b699e65415c231ace3b059">Hugging Face</a> /
            <a href="https://github.com/swiss-ai/pretrain-code">Pretrain Code</a> /
            <a href="https://github.com/swiss-ai/pretrain-data">Pretrain Data</a> /
            <a href="https://github.com/swiss-ai/posttraining">Posttrain Code</a> /
            <a href="https://github.com/swiss-ai/posttraining-data">Posttrain Data</a> /
            <a href="https://github.com/swiss-ai/evals">Evals</a>
            <p></p>
            <p>
            <span style="color: #ff6666; font-weight: bold;">The biggest fully open and compliant training run & LLM to date.</span>
            </p>
            <p>
            8B and 70B fully pretrained open-data open-weights models, multilingual in >1000 languages. Performance equivalent or better than corresponding Llama 3 sizes.
            </p>
          </td>
        </tr>

          <!-- <tr bgcolor="#ffffd0"> -->
            <td style="padding:16px;width:20%;vertical-align:middle">
              <img src="images_raghav/safety-subspaces.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2505.14185" id="MCG_journal">
              <span class="papertitle">Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study
              </span>
              </a>
              <br>
              <a href="https://kaustubhp11.github.io/">Kaustubh Ponkshe*</a>, 
              <a href="https://shaan-shah.github.io/">Shaan Shah*</a>,
              <strong>Raghav Singhal*</strong>, 
              <a href="https://praneeth.mit.edu/">Praneeth Vepakomma</a>
              <br>
              <span style="color: #666; font-size: 0.9em;">Abridged at</span> <em style="color: #004499;">Lock-LLM @ NeurIPS 2025</em>
              <br>
              <span style="color: #888; font-style: italic; font-size: 0.9em;">Under review</span>
              <br>
              <a href="https://github.com/CERT-Lab/safety-subspaces">code</a> /
              <a href="https://arxiv.org/abs/2505.14185">arXiv</a>
              <p></p>
              <p>
              We show that safety alignment in LLMs is not confined to distinct subspaces (but rather, highly entangled with general abality directions), thus fundamentally challenging the foundation of subspace-based defenses.
              </p>
            </td>
          </tr>

        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/fedex-lora-2.png" alt="PontTuset" width="100%" height="100%" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2410.09432" id="MCG_journal">
            <span class="papertitle">FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models
            </span>
            </a>
            <br>
            <strong>Raghav Singhal*</strong>, 
            <a href="https://kaustubhp11.github.io/">Kaustubh Ponkshe*</a>, 
            <a href="https://praneeth.mit.edu/">Praneeth Vepakomma</a>
            <br>
	<em style="color: #004499;">ACL 2025</em> - <em style="color: #ff6666; font-weight: bold;">Oral (Top 2.2% of submitted papers) </em>
            <br>
            <a href="https://raghavsinghal10.github.io/fedex-lora_page/">project page</a> /
            <a href="https://github.com/RaghavSinghal10/fedex-lora">code</a> /
            <a href="https://arxiv.org/abs/2410.09432">arXiv</a>
            <p></p>
            <p>
            We achieve exact aggregation in distributed fine-tuning of LLMs, consistently improving over SOTA.
            </p>
          </td>
        </tr>
                      <!-- <tr bgcolor="#ffffd0"> -->
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/abba.png" alt="PontTuset" width="160" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2505.14238" id="MCG_journal">
            <span class="papertitle">ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models
            </span>
            </a>
            <br>
            <strong>Raghav Singhal*</strong>, 
            <a href="https://kaustubhp11.github.io/">Kaustubh Ponkshe*</a>, 
            Rohit Vartak*, 
            <a href="https://praneeth.mit.edu/">Praneeth Vepakomma</a>
            <br>
            <span style="color: #666; font-size: 0.9em;">Abridged at</span> <em style="color: #004499;">ES-FOMO @ ICML 2025</em> - <em style="color: #ff6666; font-weight: bold;">Spotlight (Top 9.5% of accepted papers)</em>
            <br>
            <span style="color: #888; font-style: italic; font-size: 0.9em;">Under review</span>
            <br>
	    <a href="https://rohit01-zoey.github.io/abba-page/">project page</a> /
            <a href="https://github.com/CERT-Lab/abba">code</a> /
            <a href="https://arxiv.org/abs/2505.14238">arXiv</a>
            <p></p>
            <p>
            We introduce ABBA, a PEFT method that enhances expressivity by decoupling low-rank updates from pre-trained weights via a Hadamard product, consistently improving over SOTA methods.
            </p>
          </td>
        </tr>




		          <!-- <tr bgcolor="#ffffd0"> -->
          <td style="padding:16px;width:20%;vertical-align:middle;text-align: center;">
            <img src="images_raghav/lora-sb-2.jpg" alt="PontTuset" width="140" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2411.19557" id="MCG_journal">
            <span class="papertitle">Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning
            </span>
            </a>
            <br>
            <a href="https://kaustubhp11.github.io/">Kaustubh Ponkshe*</a>, 
            <strong>Raghav Singhal*</strong>, 
            <a href="https://eduardgorbunov.github.io/">Eduard Gorbunov</a>, 
            <a href="https://faculty.cc.gatech.edu/~atumanov/">Alexey Tumanov</a>, 
            <a href="https://sites.google.com/view/samuelhorvath">Samuel Horvath</a>, 
            <a href="https://praneeth.mit.edu/">Praneeth Vepakomma</a>
            <br>
            <span style="color: #666; font-size: 0.9em;">Abridged at</span> <em style="color: #004499;">SCOPE @ ICLR 2025</em>
            <br>
            <span style="color: #888; font-style: italic; font-size: 0.9em;">Under review</span>
            <br>
            <a href="https://raghavsinghal10.github.io/lora-sb-page/">project page</a> /
            <a href="https://github.com/CERT-Lab/lora-sb">code</a> /
            <a href="https://arxiv.org/abs/2411.19557">arXiv</a>
            <p></p>
            <p>
            We provably achieve the best approximation of full fine-tuning in low-rank spaces solely through clever initialization, outperforming LoRA while using up to 90x fewer parameters.
            </p>
          </td>
        </tr>
		  
            <!-- <tr bgcolor="#ffffd0"> -->
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/fed-sb-2.png" alt="PontTuset" width="160" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2502.15436" id="MCG_journal">
            <span class="papertitle">Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning
            </span>
            </a>
            <br>
            <strong>Raghav Singhal*</strong>, 
            <a href="https://kaustubhp11.github.io/">Kaustubh Ponkshe*</a>, 
            Rohit Vartak, 
            <a href="http://www.varshney.csl.illinois.edu/">Lav Varshney</a>, 
            <a href="https://praneeth.mit.edu/">Praneeth Vepakomma</a>
            <br>
            <span style="color: #666; font-size: 0.9em;">Abridged at</span> <em style="color: #004499;">ES-FOMO @ ICML 2025</em>
            <br>
            <span style="color: #888; font-style: italic; font-size: 0.9em;">Under review</span>
            <br>
            <a href="https://rohit01-zoey.github.io/fed-sb-page/">project page</a> /
            <a href="https://github.com/CERT-Lab/fed-sb">code</a> /
            <a href="https://arxiv.org/abs/2502.15436">arXiv</a>
            <p></p>
            <p>
            We set a new Pareto frontier for distributed fine-tuning of LLMs, achieving SOTA performance, stronger privacy guarantees, and up to 230x lower communication costs.
            </p>
          </td>
        </tr>




        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/m3col-2.jpg" alt="PontTuset" width="160" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2409.17777" id="MCG_journal">
            <span class="papertitle">M3CoL: Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification
            </span>
            </a>
            <br>
            Raja Kumar*, 
            <strong>Raghav Singhal*</strong>, 
            Pranamya Kulkarni,
            <a href="https://research.monash.edu/en/persons/deval-mehta">Deval Mehta</a>, 
            <a href="https://www.linkedin.com/in/kshitij-jadhav-64083b110">Kshitij Jadhav</a>
            <br>
            <em style="color: #004499;">TMLR</em>
            <br>
            <a href="https://raghavsinghal10.github.io/m3col_page/">project page</a> /
            <a href="https://github.com/RaghavSinghal10/M3CoL">code</a> /
            <a href="https://arxiv.org/abs/2409.17777">arXiv</a>
            <p></p>
            <p>
            We introduce a multimodal mixup-based contrastive learning framework that effectively captures shared relations across modalities, enabling robust multimodal representation learning.
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/qat.jpg" alt="PontTuset" width="160" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2503.01297" id="MCG_journal">
            <span class="papertitle">Regularization-based Framework for Quantization-, Fault- and Variability-Aware Training
            </span>
            </a>
            <br>
            Anmol Biswas*,
            <strong>Raghav Singhal*</strong>, 
            Sivakumar Elangovan,
            Shreyas Sabnis,
            <a href="https://www.ee.iitb.ac.in/~udayanresearch/">Udayan Ganguly</a>
            <br>
            <span style="color: #666; font-size: 0.9em;">Abridged at</span> <em style="color: #004499;">MLNCP @ NeurIPS 2024</em>
            <br>
            <span style="color: #888; font-style: italic; font-size: 0.9em;">Under review</span>
            <br>
            <a href="https://arxiv.org/abs/2503.01297">arXiv</a>
            <p></p>
            <p>
            We develop a learnable, non-uniform quantization-aware training framework that boosts efficiency and reliability of AI models deployed on low-power edge devices.
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images_raghav/rf.png" alt="PontTuset" width="160" style="border-style: none">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="https://dl.acm.org/doi/abs/10.1145/3584954.3584996" id="MCG_journal">
            <span class="papertitle">Translation and Scale Invariance for Event-Based Object Tracking
            </span>
            </a>
            <br>
            <a href="https://jepedersen.dk/">Jens Egholm Pedersen</a>, 
            <strong>Raghav Singhal</strong>, 
            <a href="https://tum.neurocomputing.systems/en/research/">Jörg Conradt</a>
            <br>
            <em style="color: #004499;">NICE 2023</em>
            <br>
            <a href="https://github.com/Jegp/coordinate-regression">code</a> /
            <a href="https://dl.acm.org/doi/abs/10.1145/3584954.3584996">paper</a>
            <p></p>
            <p>
            We train an extremely low-power SNN capable of accurate temporal regression, achieving ANN-level performance and faster convergence, directly portable to neuromorphic hardware.
            </p>
          </td>
        </tr>

<!-- 
      <tr>
        <td style="padding:16px;width:20%;vertical-align:middle">
          <img src="images/PABMM2015.jpg" alt="PontTuset" width="160" style="border-style: none">
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">
            <span class="papertitle">Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</span>
          </a>
          <br>
          <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
          <br>
          <em>TPAMI</em>, 2017
          <br>
          <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
          <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
          <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a>
          <p></p>
          <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p>
          <p>This paper subsumes our CVPR 2014 paper.</p>
        </td>
      </tr>



    
    <tr onmouseout="power_stop()" onmouseover="power_start()" bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='power_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/power.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/power.png' width="160">
        </div>
        <script type="text/javascript">
          function power_start() {
            document.getElementById('power_image').style.opacity = "1";
          }

          function power_stop() {
            document.getElementById('power_image').style.opacity = "0";
          }
          power_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://x.com/jon_barron/status/1891918200931061996">
			<span class="papertitle">A Power Transform
</span>
        </a>
        <br>
				<strong>Jonathan T. Barron</strong>
        <br>
        <em>arXiv</em>, 2025
        <br>
        <a href="https://x.com/jon_barron/status/1891918200931061996">tweet</a>
        /
        <a href="https://arxiv.org/abs/2502.10647">arXiv</a>
        <p></p>
        <p>
				A slight tweak to the Box-Cox power transform generalizes a variety of curves, losses, kernel functions, probability distributions, bump functions, and neural network activation functions.
        </p>
      </td>
    </tr>


    <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
          <source src="images/r2r.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/r2r.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for
3D Reconstruction under Extreme Illumination Variation</span>
        </a>
        <br>
        <a href="https://hadizayer.github.io/">Hadi Alzayer</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
				<strong>Jonathan T. Barron</strong>, 
        <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>, 
        <a href="https://dorverbin.github.io/">Dor Verbin</a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://relight-to-reconstruct.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a>
        <p></p>
        <p>
				Images taken under extreme illumination variation can be made consistent with diffusion, and this enables high-quality 3D reconstruction.
        </p>
      </td>
    </tr>


    <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='simvs_image'><video  width=100% muted autoplay loop>
          <source src="images/simvs.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/simvs.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://alextrevithick.github.io/simvs/">
          <span class="papertitle">SimVS: Simulating World Inconsistencies for Robust View Synthesis</span>
        </a>
        <br>
        <a href="https://alextrevithick.github.io/">Alex Trevithick</a>,
        <a href="https://scholar.google.com/citations?user=-KSDNZQAAAAJ&hl=en">Roni Paiss</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu</a>,
        <a href="https://hadizayer.github.io/">Hadi Alzayer</a>,
        <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
        <a  href="https://poolio.github.io/">Ben Poole</a>,
				<strong>Jonathan T. Barron</strong>, 
        <a href="https://holynski.org/">Aleksander Holynski</a>,
        <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://alextrevithick.github.io/simvs/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.07696">arXiv</a>
        <p></p>
        <p>
        Simulating the world with video models lets you make inconsistent captures consistent.
        </p>
      </td>
    </tr>



    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()" bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat4d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cat4d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://cat-4d.github.io/">
			<span class="papertitle">CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models
</span>
        </a>
        <br>
				<a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu</a>,
				<a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
				<a href="https://poolio.github.io/">Ben Poole</a>,
				<a href="https://alextrevithick.github.io/">Alex Trevithick</a>,
				<a href="https://www.cs.columbia.edu/~cxz/index.htm/">Changxi Zheng</a>,
				<strong>Jonathan T. Barron</strong>,
				<a href="https://holynski.org/">Aleksander Holynski</a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://cat-4d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2411.18613">arXiv</a>
        <p></p>
        <p>
				An approach for turning a video into a 4D radiance field that can be rendered in real-time. When combined with a text-to-video model, this enables text-to-4D.
        </p>
      </td>
    </tr>


    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/ever_after.png' width=100%>
					</div>
          <img src='images/ever_before.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://half-potato.gitlab.io/posts/ever/">
			<span class="papertitle">EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis
</span>
        </a>
        <br>
				<a href="https://half-potato.gitlab.io/">Alexander Mai</a>, 
				<a href="https://phogzone.com/">Peter Hedman</a>,
				<a href="https://grgkopanas.github.io/">George Kopanas</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://scholar.google.com/citations?user=ozNFrecAAAAJ&hl=en">David Futschik</a>,
        <a href="https://xharlie.github.io/">Qiangeng Xu</a>,
        <a href="https://jacobsschool.ucsd.edu/faculty/profile?id=253">Falko Kuester</a>,
				<strong>Jonathan T. Barron</strong>,
        <a href="https://www.zhangyinda.com/">Yinda Zhang</a>
				<br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://half-potato.gitlab.io/posts/ever/">project page</a>
        /
        <a href="https://arxiv.org/abs/2410.01804">arXiv</a>
        <p></p>
        <p>
				Raytracing constant-density ellipsoids yields more accurate and flexible radiance fields than splatting Gaussians, and still runs in real-time.
        </p>
      </td>
    </tr>


    <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()" bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cat3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat3d_start() {
            document.getElementById('cat3d_image').style.opacity = "1";
          }

          function cat3d_stop() {
            document.getElementById('cat3d_image').style.opacity = "0";
          }
          cat3d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">CAT3D: Create Anything in 3D with Multi-View Diffusion Models
</span>
        </a>
        <br>
				<a href="https://ruiqigao.github.io/">Ruiqi Gao</a>*,
        <a href="https://holynski.org/">Aleksander Holynski</a>*, 
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>, 
				<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
				<strong>Jonathan T. Barron</strong>,
        <a href="https://poolio.github.io/">Ben Poole</a>*

        <br>
        <em>NeurIPS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://cat3d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
        <p></p>
        <p>
				A single model built around diffusion and NeRF that does text-to-3D, image-to-3D, and few-view reconstruction, trains in 1 minute, and renders at 60FPS in a browser.
        </p>
      </td>
    </tr>


    <tr onmouseout="nerfcasting_stop()" onmouseover="nerfcasting_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nerfcasting_image'><video  width=100% muted autoplay loop>
          <source src="images/nerfcasting.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nerfcasting.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nerfcasting_start() {
            document.getElementById('nerfcasting_image').style.opacity = "1";
          }

          function nerfcasting_stop() {
            document.getElementById('nerfcasting_image').style.opacity = "0";
          }
          nerfcasting_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections</span>
        </a>
        <br>
				
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://phogzone.com/">Peter Hedman</a>,
				<a href="https://benattal.github.io/">Benjamin Attal</a>, <br>
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH Asia</em>, 2024
        <br>
        <a href="https://nerf-casting.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.14871">arXiv</a>
        <p></p>
        <p>
        Carefully casting reflection rays lets us synthesize photorealistic specularities in real-world scenes.
        </p>
      </td>
    </tr>


    <tr onmouseout="flash_cache_stop()" onmouseover="flash_cache_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='flash_cache_image'><video  width=100% muted autoplay loop>
          <source src="images/flash_cache.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/flash_cache.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function flash_cache_start() {
            document.getElementById('flash_cache_image').style.opacity = "1";
          }

          function flash_cache_stop() {
            document.getElementById('flash_cache_image').style.opacity = "0";
          }
          flash_cache_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://benattal.github.io/flash-cache/">
          <span class="papertitle">Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering</span>
        </a>
        <br>
				<a href="https://benattal.github.io/">Benjamin Attal</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>, <br>
				<strong>Jonathan T. Barron</strong>,
        <a href="https://www.cs.cmu.edu/~motoole2/">Matthew O'Toole</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>
        <br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://benattal.github.io/flash-cache/">project page</a>
        /
        <a href="TODO">arXiv</a>
        <p></p>
        <p>
          A more physically-accurate inverse rendering system based on radiance caching for recovering geometry, materials, and lighting from RGB images of an object or scene.
        </p>
      </td>
    </tr>



    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="images/nuvo.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nuvo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <span class="papertitle">Nuvo: Neural UV Mapping for Unruly 3D Representations</span>
        </a>
        <br>
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
        <p></p>
        <p>
        Neural fields let you recover editable UV mappings for the challenging geometries produced by NeRF-like models.
        </p>
      </td>
    </tr>


    <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bog_image'><video  width=100% muted autoplay loop>
          <source src="images/bog.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bog.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://creiser.github.io/binary_opacity_grid/">
          <span class="papertitle">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis
</span>
        </a>
        <br>
				<a href="https://creiser.github.io/">Christian Reiser</a>,
				<a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
				<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://dorverbin.github.io/">Dor Verbin</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<strong>Jonathan T. Barron</strong>,
				<a href="https://phogzone.com/">Peter Hedman</a>*,
				<a href="https://www.cvlibs.net/">Andreas Geiger</a>*		
        <br>
        <em>SIGGRAPH</em>, 2024
        <br>
        <a href="https://creiser.github.io/binary_opacity_grid/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=2TPUmGRg8bM">video</a>
        /
        <a href="https://arxiv.org/abs/2402.12377">arXiv</a>
        <p></p>
        <p>
        Applying anti-aliasing to a discrete opacity grid lets you render a hard representation into a soft image, and this enables highly-detailed mesh recovery.
        </p>
      </td>
    </tr>

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/smerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smerf.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://smerf-3d.github.io/">
          <span class="papertitle">SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</span>
        </a>
        <br>
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>,
		<a href="https://phogzone.com/">Peter Hedman*</a>,
		<a href="https://creiser.github.io/">Christian Reiser</a>,
		<a href="">Peter Zhizhin</a>,
		<a href="">Jean-François Thibert</a>,
        <a href="https://lucic.ai/">Mario Lučić</a>,
        <a href="https://szeliski.org/">Richard Szeliski</a>,
		<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH</em>, 2024 &nbsp <font color="red"><strong>(Honorable Mention)</strong></font>
        <br>
        <a href="https://smerf-3d.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
      </td>
    </tr>
	



  <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
    <td style="padding:16px;width:20%;vertical-align:middle">
      <div class="one">
        <div class="two" id='eclipse_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/eclipse_after.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/eclipse_before.jpg' width="160">
      </div>
      <script type="text/javascript">
        function eclipse_start() {
          document.getElementById('eclipse_image').style.opacity = "1";
        }

        function eclipse_stop() {
          document.getElementById('eclipse_image').style.opacity = "0";
        }
        eclipse_stop()
      </script>
    </td>
    <td style="padding:8px;width:80%;vertical-align:middle">
      <a href="https://dorverbin.github.io/eclipse">
        <span class="papertitle">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</span>
      </a>
      <br>
      <a href="https://dorverbin.github.io/">Dor Verbin</a>,
      <a href="https://bmild.github.io/">Ben Mildenhall</a>,
      <a href="https://phogzone.com/">Peter Hedman</a>, <br>
      <strong>Jonathan T. Barron</strong>,
      <a href="Todd Zickler">Todd Zickler</a>,
      <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
      <br>
      <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
      <br>
      <a href="https://dorverbin.github.io/eclipse">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
      /
      <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
      <p></p>
      <p>
      Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and materials.
      </p>
    </td>
  </tr>


  <tr onmouseout="recon_stop()" onmouseover="recon_start()" bgcolor="#ffffd0">
    <td style="padding:16px;width:20%;vertical-align:middle">
      <div class="one">
        <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/recon.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/recon.png' width="160">
      </div>
      <script type="text/javascript">
        function recon_start() {
          document.getElementById('recon_image').style.opacity = "1";
        }

        function recon_stop() {
          document.getElementById('recon_image').style.opacity = "0";
        }
        recon_stop()
      </script>
    </td>
    <td style="padding:8px;width:80%;vertical-align:middle">
      <a href="https://reconfusion.github.io/">
		<span class="papertitle">ReconFusion: 3D Reconstruction with Diffusion Priors</span>
      </a>
      <br>
      <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu*</a>,
	<a href="https://bmild.github.io/">Ben Mildenhall*</a>,
      <a href="https://henzler.github.io/">Philipp Henzler</a>,
      <a href="https://keunhong.com/">Keunhong Park</a>,
      <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
      <a href="https://scholar.google.com/citations?user=_pKKv2QAAAAJ&hl=en/">Daniel Watson</a>,
      <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
      <a href="https://dorverbin.github.io/">Dor Verbin</a>,
	<strong>Jonathan T. Barron</strong>,
      <a href="https://poolio.github.io/">Ben Poole</a>,
      <a href="https://holynski.org/">Aleksander Holynski*</a>
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://reconfusion.github.io/">project page</a>
      /
      <a href="https://arxiv.org/abs/2312.02981">arXiv</a>
      <p></p>
      <p>
      Using a multi-image diffusion model as a regularizer lets you recover high-quality radiance fields from just a handful of images.
      </p>
    </td>
  </tr>

  <tr onmouseout="shinobi_stop()" onmouseover="shinobi_start()">
    <td style="padding:16px;width:20%;vertical-align:middle">
      <div class="one">
        <div class="two" id='shinobi_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/shinobi.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/shinobi.jpg' width="160">
      </div>
      <script type="text/javascript">
        function shinobi_start() {
          document.getElementById('shinobi_image').style.opacity = "1";
        }

        function shinobi_stop() {
          document.getElementById('shinobi_image').style.opacity = "0";
        }
        shinobi_stop()
      </script>
    </td>
    <td style="padding:8px;width:80%;vertical-align:middle">
      <a href="https://shinobi.aengelhardt.com/">
        <span class="papertitle">SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-Wild</span>
      </a>
      <br>
			
			<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/">Andreas Engelhardt</a>, 
			<a href="https://amitraj93.github.io/">Amit Raj</a>, 
			<a href="https://markboss.me/">Mark Boss</a>, 
			<a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>, 
			<a href="https://abhishekkar.info/">Abhishek Kar</a>, 
			<a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
			<a href="https://deqings.github.io/">Deqing Sun</a>, 
			<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
      <strong>Jonathan T. Barron</strong>,
			<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/">Hendrik P.A. Lensch</a>, 
			<a href="https://varunjampani.github.io/">Varun Jampani</a>
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://shinobi.aengelhardt.com/">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=m_5kvtlDnl4">video</a>
      /
      <a href="https://arxiv.org/abs/2401.10171">arXiv</a>
      <p></p>
      <p>
      A class-agnostic inverse rendering solution for turning in-the-wild images of an object into a relightable 3D asset.
      </p>
    </td>
  </tr>
	

    <tr onmouseout="internerf_stop()" onmouseover="internerf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='internerf_image'>
					  <img src='images/internerf_after.jpg' width=100%>
					</div>
          <img src='images/internerf_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function internerf_start() {
            document.getElementById('internerf_image').style.opacity = "1";
          }

          function internerf_stop() {
            document.getElementById('internerf_image').style.opacity = "0";
          }
          internerf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.11737">
          <span class="papertitle">InterNeRF: Scaling Radiance Fields via Parameter Interpolation</span>
        </a>
        <br>
		<a href="https://clintonjwang.github.io/">Clinton Wang</a>,
		<a href="https://phogzone.com/">Peter Hedman</a>,
		<a href="https://people.csail.mit.edu/polina/">Polina Golland</a>,
		<strong>Jonathan T. Barron</strong>,
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
        <br>
        <em>CVPR Neural Rendering Intelligence</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2406.11737">arXiv</a>
        <p></p>
        <p>
        Parameter interpolation enables high-quality large-scale scene reconstruction and out-of-core training and rendering.
        </p>
      </td>
    </tr>


<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
    <div class="two" id='difsurvey_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/difsurvey_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/difsurvey_image.jpg' width="160">
    </div>
    <script type="text/javascript">
      function difsurvey_start() {
        document.getElementById('difsurvey_image').style.opacity = "1";
      }

      function difsurvey_stop() {
        document.getElementById('difsurvey_image').style.opacity = "0";
      }
      difsurvey_stop()
    </script>
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2310.07204">
      <span class="papertitle">State of the Art on Diffusion Models for Visual Computing
</span>
    </a>
    <br>
	<a href="https://ryanpo.com/">Ryan Po</a>,
	<a href="https://yifita.netlify.app/">Wang Yifan</a>,
	<a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>,
	<a href="https://kfiraberman.github.io/">Kfir Aberman</a>,
	<strong>Jonathan T. Barron</strong>,
	<a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
	<a href="https://ericryanchan.github.io/">Eric Ryan Chan</a>,
	<a href="https://www.weizmann.ac.il/math/dekel/home">Tali Dekel</a>,
	<a href="https://holynski.org/">Aleksander Holynski</a>,
	<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
	<a href="https://tml.stanford.edu/">C. Karen Liu</a>,
	<a href="https://lingjie0206.github.io/">Lingjie Liu</a>,
	<a href="https://bmild.github.io/">Ben Mildenhall</a>,
    <a href="https://www.niessnerlab.org/">Matthias Nießner</a>,
	<a href="https://ommer-lab.com/people/ommer/">Björn Ommer</a>,
	<a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>,
	<a href="https://peterwonka.net/">Peter Wonka</a>,
    <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
    <br>
	<em>Eurographics State-of-the-Art Report<em>, 2024
    <br>
    <p></p>
    <p>
    A survey of recent progress in diffusion models for images, videos, and 3D.
    </p>
  </td>
</tr>          

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/camp.png' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://camp-nerf.github.io/">
          <span class="papertitle">CamP: Camera Preconditioning for Neural Radiance Fields</span>
        </a>
        <br>
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>
        <br>
        <em>SIGGRAPH Asia</em>, 2023
        <br>
        <a href="https://camp-nerf.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2308.10902">arXiv</a>
        <p></p>
        <p>
        Preconditioning based on camera parameterization helps NeRF and camera extrinsics/intrinsics optimize better together.
        </p>
      </td>
    </tr>

    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</span>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
          <br>
          <a href="http://jonbarron.info/zipnerf">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
          /
          <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
          <p></p>
          <p>
          Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x.
          </p>
        </td>
      </tr>
      
      
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://dreambooth3d.github.io/">
            <span class="papertitle">DreamBooth3D: Subject-Driven Text-to-3D Generation</span>
          </a>
          <br>
          
  <a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
          <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://dreambooth3d.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2303.13508">arXiv</a>
          <p></p>
          <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
        </td>
      </tr>

      

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://bakedsdf.github.io/">
            <span class="papertitle">BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</span>
          </a>
          <br>
          <a href="https://lioryariv.github.io/">Lior Yariv*</a>,
          <a href="https://phogzone.com/">Peter Hedman*</a>,
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,  <br>
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://bakedsdf.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a>
          /
          <a href="https://arxiv.org/abs/2302.14859">arXiv</a>
          <p></p>
          <p>
          We use SDFs to bake a NeRF-like model into a high quality mesh and do real-time view synthesis.
          </p>
        </td>
      </tr>


      <tr onmouseout="merf_stop()" onmouseover="merf_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/merf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/merf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function merf_start() {
              document.getElementById('merf_image').style.opacity = "1";
            }

            function merf_stop() {
              document.getElementById('merf_image').style.opacity = "0";
            }
            merf_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://merf42.github.io/">
            <span class="papertitle">MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes</span>
          </a>
          <br>
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://merf42.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=3EACM2JAcxc">video</a>
          /
          <a href="https://arxiv.org/abs/2302.12249">arXiv</a>
          <p></p>
          <p>
          We use volumetric rendering with a sparse 3D feature grid and 2D feature planes to do real-time view synthesis.
          </p>
        </td>
      </tr> -->



      <!-- <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='alignerf_image'>
              <img src='images/alignerf_after.jpg' width="160"></div>
            <img src='images/alignerf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function alignerf_start() {
              document.getElementById('alignerf_image').style.opacity = "1";
            }

            function alignerf_stop() {
              document.getElementById('alignerf_image').style.opacity = "0";
            }
            alignerf_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://yifanjiang19.github.io/alignerf">
            <span class="papertitle">AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training</span>
          </a>
          <br>
          <a href="https://yifanjiang.net/">Yifan Jiang</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>, 
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://ir1d.github.io/">Dejia Xu</a>, <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
          <a href="https://tianfan.info/">Tianfan Xue</a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="https://yifanjiang19.github.io/alignerf">project page</a>
          /
          <a href="https://arxiv.org/abs/2211.09682">arXiv</a>
          <p></p>
          <p>
          Accounting for misalignment due to scene motion or calibration errors improves NeRF reconstruction quality.
          </p>
        </td>
      </tr> -->

            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code taken from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
